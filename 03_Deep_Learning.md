
# 3. Deep Learning

## 3.1 Overview

#### Paper

- <https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap>

    Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!

- [Recent Advances in Deep Learning: An Overview - Bangladesh2018](https://arxiv.org/abs/1807.08169v1)

    这篇综述论文列举了近年来深度学习的重要研究成果，从方法、架构、正则化、优化技术等方面进行概述。

- [The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches - 2018](https://arxiv.org/abs/1803.01164)

    **Chinese**: [从起源到具体算法，这篇深度学习综述论文送给你](https://yq.aliyun.com/articles/552398)

- <https://github.com/robertsdionne/neural-network-papers>

    Surveys, Books, Datasets, Pretrained Models, Frameworks, NLP, CNN, RNN, CRNN, Boltzman, RL, Optimization, Hardware, Clustering, etc.

- [Mathematics of Deep Learning - 2017](https://arxiv.org/abs/1712.04741)

    **Chinese**: [揭秘深度学习成功的数学原因：从全局最优性到学习表征不变性](http://www.sohu.com/a/210846652_465975)

- [Layer rotation: a surprisingly powerful indicator of generalization in deep networks? - FNRS2019](https://arxiv.org/abs/1806.01603)

    层旋转

#### Code

- 【Great!!!】<https://github.com/rasbt/deeplearning-models>

    大量深度学习架构、模型和Tips，使用Tensorflow和PyTorch两种语言Double实现！

- 【Great!!!】[超全！深度学习最常见的 26 个模型练手项目汇总](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247489258&idx=1&sn=aa6690482375bdad09cdbca80ed06068)

- <https://github.com//RedditSota/state-of-the-art-result-for-machine-learning-problems>

    This repository provides SOTA results for all machine learning problems. We do our best to keep this repository up to date.

#### Course

- Neural Network and Deep Learning - Coursera by Andrew Ng

    Language: English, Chinese, ...

    Homework: Jupyter on Coursera

    **Note**: [深度学习笔记目录](http://www.ai-start.com/dl2017/)

#### Article

- [这 28 张精炼图，将吴恩达的 deeplearning.ai 总结得恰到好处！](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247487236&idx=1&sn=4547a524c6884adfff4c13ba0287cf98)

- [如何简单形象又有趣地讲解神经网络是什么？ - 2017](https://www.zhihu.com/question/22553761#answer-9961552)

- [图文并茂的神经网络架构大盘点：从基本原理到衍生关系 - 2016](http://www.360doc.com/content/16/0915/20/13792507_591115257.shtml)

- [辨别真假数据科学家必备手册：深度学习45个基础问题（附答案）- 2017](http://mini.eastday.com/mobile/170227083107275.html)

- [过参数化、剪枝和网络结构搜索 - 2019](https://zhuanlan.zhihu.com/p/74553341)

- [神经网络多样性的意义何在？既然多层感知机在理论上已经可以拟合任何函数，为什么要有不同的形式？](https://www.zhihu.com/question/342489117)

#### Github

- 【Great】<https://github.com/sladesha/deep_learning>

    工程和学术研究中，一些应用到深度学习的思路和方法整理汇总


## 3.2 CNN

#### Paper

- [What Do We Understand About Convolutional Networks? - Canada2018](https://arxiv.org/abs/1803.08834)

    **Chinese**: [94 页论文综述卷积神经网络：从基础技术到研究前景](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247493220&idx=3&sn=a27d0aaf9210f46e614c2863ed76de3d)

#### Tool

- [3D Visualization of a CNN](http://scs.ryerson.ca/~aharley/vis/conv/)

#### Article

- [YJango的卷积神经网络——介绍 - 2019](https://zhuanlan.zhihu.com/p/27642620)

- [Convolutional Neural Networks backpropagation: from intuition to derivation - 2016](https://grzegorzgwardys.wordpress.com/2016/04/22/8/)

- [机器学习原来这么有趣！第三章:图像识别【鸟or飞机】？深度学习与卷积神经网络 - 2017](https://zhuanlan.zhihu.com/p/24524583)

- [CNN详解——反向传播过程](https://blog.csdn.net/HappyRocking/article/details/80512587)

- [卷积层和分类层，哪个更重要？ - 2019](https://www.zhihu.com/question/270245936)

- [15个CNN关键回答集锦，2019校招面试必备！- 2018](https://blog.csdn.net/qq_28168421/article/details/81713457)

- [自然语言处理中CNN模型几种常见的Max Pooling操作 - 2016](http://blog.csdn.net/malefactor/article/details/51078135)

- [变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作 - 2019](https://zhuanlan.zhihu.com/p/28749411)

- [CNN调优总结 - 2019](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247487348&idx=2&sn=b321b86e29a258e807b09f6deecaea35)

- [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning - 2019](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)

    **Chinese**: [卷积有多少种？一文读懂深度学习中的各种卷积](https://mp.weixin.qq.com/s?__biz=MzUyOTU2MjE1OA==&mid=2247486088&idx=1&sn=78af01181a293551aef468baeac1ae00)

- [四张图彻底搞懂CNN反向传播算法（通俗易懂） - 2019](https://zhuanlan.zhihu.com/p/81675803)


## 3.3 RNN

#### Article

- [解决RNN的梯度消失问题 - 2019](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247489660&idx=4&sn=be100c0a984554681937dfb9126ef92a)

- [卷积网络循环网络结合-CNN+RNN - 2018](http://www.sohu.com/a/222287668_609569)


## 3.4 LSTM/GRU

### 3.4.1 LSTM

- LSTM: [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

- [Regularizing and Optimizing LSTM Language Models - Salesforce2017](https://arxiv.org/abs/1708.02182)

    LSTM优化和训练技巧 推荐阅读！

#### Article

- [Understanding LSTM Networks - 2015](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

- [Deep Learning for NLP Best Practices blog](http://ruder.io/deep-learning-nlp-best-practices/)

- [Visualizing LSTM Networks. Part I - 2018](https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7)

    **Code**: <https://github.com/asap-report/lstm-visualisation>

    **Chinese**: [可视化LSTM网络：探索「记忆」的形成](https://baijiahao.baidu.com/s?id=1596521378161967384)


#### Practice

- [如何准备用于LSTM模型的数据并进行序列预测？（附代码）](https://www.jiqizhixin.com/articles/2018-12-18-16)


### 3.4.2 AWD-LSTM

AWD-LSTM 对 LSTM 模型进行了改进，包括在隐藏层间加入 dropout ，加入词嵌入 dropout ，权重绑定等。建议**使用 AWD-LSTM 来替代 LSTM 以获得更好的模型效果**。

[Regularizing and Optimizing LSTM Language Models - 2017](https://arxiv.org/abs/1708.02182)

#### Code

- [Github by Salesforce](https://github.com/salesforce/awd-lstm-lm)
  
- [Github by fastAI](https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py)


### 3.4.3 Adversarial LSTM

[Adversarial Training Methods for Semi-Supervised Text Classification - 2016](https://arxiv.org/abs/1605.07725)

核心思想：通过对 word Embedding 上添加噪音生成对抗样本，将对抗样本以和原始样本 同样的形式喂给模型，得到一个 Adversarial Loss，通过和原始样本的 loss 相加得到新的损失，通过优化该新 的损失来训练模型，作者认为这种方法能对 word embedding 加上正则化，避免过拟合。

#### Practice

- [文本分类实战（七）—— Adversarial LSTM 模型](https://www.cnblogs.com/jiangxinyang/p/10208363.html)


### 3.4.4 Advanced LSTM

[Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition - Alibaba2017](https://arxiv.org/abs/1710.10197)


### 3.4.5 ON-LSTM

[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks - 2019](https://arxiv.org/abs/1810.09536)

**Chineses**: [翻译](https://blog.csdn.net/zhang2010hao/article/details/91976075)

#### Code

- <https://github.com/yikangshen/Ordered-Neurons>


### 3.4.6 GRU


## 3.5 Other CNN/RNN/TCN

- QRNN: [Quasi-Recurrent Neural Networks - Salesforce2016](https://arxiv.org/abs/1611.01576)

- SRU: [Simple Recurrent Units for Highly Parallelizable Recurrence - ASAPP2018](https://arxiv.org/abs/1709.02755)

    SRU单元在本质上与QRNN单元很像。从网络构建上看，SRU单元有点像QRNN单元中的一个特例，但是又比QRNN单元多了一个直连的设计。

- IndRNN: [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN - Australia2018](https://arxiv.org/abs/1803.04831)

    将IndRNN单元配合ReLu等非饱和激活函数一起使用，会使模型表现出更好的鲁棒性。

- JANET: [The unreasonable effectiveness of the forget gate - Cambridge2018](https://arxiv.org/abs/1804.04849)

- TCN: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling - 2018](https://arxiv.org/abs/1803.01271)

    **Article**: [TCNs Take Over from RNNs for NLP Predictions - 2018](https://www.datasciencecentral.com/profiles/blogs/temporal-convolutional-nets-tcns-take-over-from-rnns-for-nlp-pred)

    **Chinese**: [时间卷积网络（TCN）在 NLP 多领域发光，RNN 或将没落](https://yq.aliyun.com/articles/593033)


## 3.6 Backpropogation

#### Article

- [你看到的最直白清晰的，神经网络中的反向传播法讲解 - 2018](https://blog.csdn.net/meyh0x5vDTk48P2/article/details/79124757)

- [你真的理解反向传播吗 - 2019](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247492079&idx=4&sn=5fb740d5324e1b1c3dec83fd8599196a)

- [为什么会有梯度爆炸和梯度消失？ - 2019](https://www.zhihu.com/question/290392414/answer/951298995)

    **YAO**: 每层神经网络大体是一次矩阵乘法+一次激活函数：$h_{t+1}=\sigma (Wh_t)$，这样基于**求导的链式法则**，$\partial h_{t+1}/\partial h_t=\sigma ^`(Wh_t)W$，第一项小于1，第二项可大于1也可小于1，两者相乘有时会有收缩效果，有时会有扩张效果，前者导致梯度消失，后者导致梯度爆炸。梯度消失或爆炸并非仅仅与激活函数有关，更与权重W有关。梯度消失的后果是无法有效完成深层网络的训练。

- [详解深度学习中的梯度消失、爆炸原因及其解决方法 - 2018](https://zhuanlan.zhihu.com/p/33006526)

    **YAO**: 

    梯度消失：LSTM, ReLU, BatchNormalization, Residual

    梯度爆炸：梯度剪切，权重正则化


## 3.6 Activation

### 3.6.1 Overview

#### Paper

- [Comparing Deep Learning Activation Functions Across NLP tasks - 2019](https://arxiv.org/abs/1901.02671)

    **Code**: <https://github.com/UKPLab/emnlp2018-activation-functions>

    **Chinese**: [21种NLP任务激活函数大比拼：你一定猜不到谁赢了](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650756158&idx=2&sn=90cb49c49be078e7406539eb93561c9e)

#### Article

- [聊一聊深度学习的activation function - 2017](https://zhuanlan.zhihu.com/p/25110450)

    **YAO**：OK   其他内容在下一Article的YAO摘要里
    
    正是由于激活函数这些非线性函数的反复叠加，使得神经网络有足够的capacity来抓取复杂的Pattern，可以逼近任意函数。
    
    推荐优先使用ReLU，同时注意初始化和学习率的设置，不建议使用Tahn(?)，尤其是Sigmoid。

- [Neural Networks: Feedforward and Backpropagation Explained & Optimization](https://mlfromscratch.com/neural-networks-explained/#/)

    **Chinese**: [从ReLU到GELU，一文概览神经网络的激活函数 - 2019](https://mp.weixin.qq.com/s/MLZ4vJjl6oXWCQn-UZ7brg)

- [Visualising Activation Functions in Neural Networks](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)

    **Chinese**: [26种神经网络激活函数可视化 - 2017](https://www.jiqizhixin.com/articles/2017-10-10-3)

    **YAO**：OK   以上2个Article的汇总

    Identity: 线性，不可单独使用，可在最终输出节点上作为激活函数用于回归任务

    **Sigmoid**: 刚开始时很流行，后来在分类任务中，正逐渐被Tanh或Log Log取代，缺点如下：
    
    - 容易Gradient Vanishing: 对于sigmoid，x较大或较小时，导数接近0，导数最大值只是0.25，意味着导数通过n层后至少会压缩为原来的1/4^n
    - 函数输出不是zero-centered: 导致模型训练收敛速度变慢。训练深度学习模型尽量使用zero-centered的输入和函数输出
    - 幂运算相对耗时：相对ReLU的thresholding而言

    Hard Sigmoid: Sigmoid的分段线性近似，比如x<=-2时是y=0的水平线，x>=2时也是水平线，介于中间是导数固定的斜线

    **Tanh**: 具有神经网络所钟爱的很多特征，完全可微分、反对称、原点对称，在分类任务中，逐渐取代Sigmoid。其优缺点如下：

    - 解决了zero-centered输出的问题
    - Gradient Vanishing和幂运算问题仍然存在

    Hard Tanh: Tanh的分段线性近似，比如x<=-1或x>=1是水平线，介于中间是导数固定的斜线，原点对称

    **ReLU**: 修正LU(线性单元)，保留了Step函数的生物学启发（只有输入超出阈值时神经元才激活：基于输入信号做门控决策），缺点是负值输入时梯度为零，学习速度很慢，甚至使神经元直接无效。
    
    - 优点有四：

      - 数学理论上解决了因激活函数导致的那种Gradient Vanishing问题
      - 计算速度非常快，只需判断输入是否大于0
      - 收敛速度远快于sigmoid和tanh，虽然输出不是zero-centered
      - ReLU会使一些神经元的输出为0，造成了网络的稀疏性，减少了参数的相互依存，缓解了过拟合（存疑？）

    - 缺点是Dead ReLU Problem，即某些神经元可能永远不会激活，相应的参数永远不被更新。原因有二，一是错误的参数初始化(比较少见)，二是学习率太高导致参数更新太大，使网络进入这种状态。解决方法有二，一是采用Xavier初始化，避免将学习率设置太大，二是使用adagrad等自动调节学习率的算法。

    - 3个ReLU的变体，共同点是为负值输入添加一个线性项，目的是为了改善Dead ReLU Problem

        - Leaky ReLU: 泄露ReLU，线性项有较小的固定的导数，能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢）
        - PReLU: 参数化ReLU，线性项的导数是在模型训练时学习得到的
        - RReLU: 随机Leaky ReLU，线性项的导数在每个节点上都是随机分配的（通常服从均匀分布）

    - 其他变体：ELU(指数LU，与上面不同，它为负值输入添加一个负指数项，导数收敛为零)，变体有SELU, SReLU

    Log Log: f(x)=1-exp(-exp(x))，有潜力替换经典的Sigmoid，该函数饱和更快，且零点值高于0.5

    **YAO**: Keras中CNN系列和Dense的默认激活函数都是Identity，实际应用中推荐优先使用ReLU？？？RNN系列的默认激活函数都是Tanh

- [Softmax](http://neuralnetworksanddeeplearning.com/chap3.html#softmax)

- [Understanding Activation Functions in Deep Learning - 2017](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)

    **YAO**:

    SWISH: $\sigma (x)=x*sigmoid(x)$，某Paper说要比ReLU效果好

- [神经网络激励函数的作用是什么？有没有形象的解释？ - 2016](https://www.zhihu.com/question/22334626)

- [谈谈激活函数Zero-Centered的问题 - 2018](https://liam0205.me/2018/04/17/zero-centered-active-function/)


### 3.6.2 ReLU

- ReLU: 正值时斜率为1，负值时斜率为0

- Leaky ReLU: 给所有负值赋予一个固定的非零斜率

- PReLU: Parametric ReLU，负值对应的斜率是根据数据来获得最优值

- RReLU: Randomized ReLU，负值对应的斜率在训练时是均匀分布中随机的，在测试时是固定的

有学者将当前最优的两类CNN结合不同的激活函数在CIFAR-10,CIFAR-100和NDSB数据集上做实验，评价四种激活函数的优劣。 实验结果表明Leaky ReLU取较大的alpha准确率更好。Parametric ReLU很容易在小数据集上过拟合（训练集上错误率最低，测试集上不理想），但依然比ReLU好。RReLU效果较好，实验表明它可以克服模型过拟合，这可能由于alpha选择的随机性。在实践中， Parametric ReLU 和 Randomized ReLU 都是可取的。

#### Article

- [ReLu(Rectified Linear Units)激活函数 - 2015](https://www.cnblogs.com/neopenx/p/4453161.html)

- [为什么ReLU比Sigmoid在很多场合都要结果好 - 2018](https://zhuanlan.zhihu.com/p/30087747)

- [人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid?](https://www.zhihu.com/question/29021768)


### 3.6.3 GELU

[Gaussian Error Linear Units - UCB2016](https://arxiv.org/abs/1606.08415)

#### Article

- [超越ReLU却鲜为人知，3年后被挖掘：BERT、GPT-2等都在用的激活函数 - 2019](https://mp.weixin.qq.com/s/xEau6wTtB4a9GT6HgPIRNQ)


## 3.7 Loss Function

#### Paper

- [Visualizing the Loss Landscape of Neural Nets - Maryland2017](https://arxiv.org/abs/1712.09913)

    **Chinese**: [马里兰大学论文：可视化神经网络的损失函数](https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/78966979)

#### Library

- <https://github.com/CyberZHG/keras-losses> (Keras)

#### Article

- [PyTorch Official API: Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)

    **YAO**: OK

    通用参数：reduce和reduction，一般损失函数都是直接计算batch的数据，返回的Loss结果是维度为<batch_size, >的向量L

    - reduce=False: reduction无效，返回L
    - reduce=True: 若reduction='mean'，返回mean(L)，若reduction='sum'，返回sum(L); 默认值是reduce=True, reduction='mean'
    
    回归：

    - L1Loss: L1范数损失，MAE损失
    - MSELoss: 均方误差损失
    - SmoothL1Loss: 又叫Huber Loss，分段函数，delta=y1-y0，loss=delta^2/2 if |delta|<1 else |delta|-0.5，所以相对于MSELoss，对outliers没那么敏感
    
    二分类：
    
    - BCELoss: 二进制交叉熵损失，不自带sigmoid，需要额外添加
    - BCEWithLogitsLoss: = Sigmoid + BCELoss，合并了两者，比两者分别单独时在数值上更稳定，因为合并后可利用log-sum-exp技巧
    - SoftMarginLoss: 二分类的Logistic损失

    多分类：支持不均衡数据的各类别权重分配
    
    - NLLLoss: 负对数似然损失，不自带LogSoftmax，
    - CrossEntropyLoss: = LogSoftmax + NLLLoss，交叉熵损失
    - MultiMarginLoss: 多分类的Hinge损失  # TODO 与上面2个的区别？？？ SVM Loss？？

    多标签分类：

    - MultiLabelMarginLoss: MultiMarginLoss在多标签任务上的拓展  # TODO 用于多标签多分类 ？？？注意PyTorch API文档略微有点坑？
    - MultiLabelSoftMarginLoss: # TODO 同MultiLableMarginLoss，区别在于y的类型是FloatTensor？？？

    序列标注：

    - CTCLoss：连接时序分类，处理连接非对齐的时间序列数据，主要是解决Label和Output不对齐的问题，优点是不用强制对齐Label且Label可变长，仅需输入序列和监督标签序列即可训练，主要应用于场景文本识别、语音识别、手写识别等。不自带LogSoftmax，需要额外添加

    排序：

    - MarginRankingLoss: 用于Pairwise排序，对于<x1, x2, y>, y=1表示x1>x2，y=-1表示x1<x2。 # TODO 或者说，用于评估相似度的？

    匹配：

    - HingeEmbeddingLoss: 用于Pairwise相似度计算，对于<x, y>，y取值1或-1，x表示2个输入的曼哈顿距离
    - CosineEmbeddingLoss: 用于Pairwise相似度计算，对于<x1, x2, y>，y取值1或-1，x表示2个输入的Cosine距离

    其他
    
    - KLDivLoss: KL散度损失，不自带Logits
    - TripletMarginLoss: 三元组Loss，对于<x1, x2, x3>，对于度量样本间的相对相似度

- [从最优化的角度看待Softmax损失函数 - 2019](https://zhuanlan.zhihu.com/p/45014864)

- [Softmax理解之二分类与多分类 - 2019](https://zhuanlan.zhihu.com/p/45368976)

- [神经网络中，设计loss function有哪些技巧](https://www.zhihu.com/question/268105631)

- [深度学习中loss和accuracy的关系](https://www.zhihu.com/question/264892967)


## 3.8 Regularization

### 3.8.1 Overview

#### Article

- [An Overview of Regularization Techniques in Deep Learning (with Python code) - 2018](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)

    **Chinese**: [深度学习中的正则化技术概述（附Python+keras实现代码）](https://mp.weixin.qq.com/s?__biz=MzUyOTU2MjE1OA==&mid=2247485921&idx=1&sn=b5a9320c0e7a89679de7695f3406bec7)

    **YAO**:



### 3.8.2 L1 & L2

- L1: 取模型所有参数的绝对值 $\lambda|W|$ 作为惩罚项加入到目标函数中。L1会使得权重向量变得稀疏(非常接近零)

- L2: 最常用，取模型所有参数的平方级 $\lambda W^2 /2$ 作为惩罚项加入到目标函数中。L2对尖峰向量的惩罚很强，并且倾向于分散权重的向量。


### 3.8.3 Dropout

有种说法是，有了Dropout，相当于一个N节点的模型，变成了2^N个模型的集合。机理是：消除了网络对一些节点的依赖，增强了泛化能力。类似于生物界中，无性繁殖 VS 有性繁殖

- Dropout: 训练时对神经网络节点进行子采样(有些子节点在向前向后连接时不考虑)，测试时Dropout不使用

    - [Improving neural networks by preventing co-adaptation of feature detectors - Toronto2012](https://arxiv.org/abs/1207.0580)
        
        提出了Dropout

    - [ImageNet Classification with Deep Convolutional Neural Networks - Toronto2012](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
    
        提出了AlexNet，使用了Dropout
    
    - [Improving Neural Networks with Dropout - Toronto2013](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)

    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting - Toronto2014](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    
    - [Dropout as data augmentation - Canada2016](https://arxiv.org/abs/1506.08700)

- [Recurrent Neural Network Regularization - NYU2015](https://arxiv.org/abs/1409.2329)

    How to correctly apply Dropout to RNNs  推荐阅读！

- Targeted Dropout: [Targeted Dropout - OX2018](https://openreview.net/pdf?id=HkghWScuoQ)

    **Code**: <https://github.com/for-ai/TD/tree/master/models>

    **Code**: <https://github.com/CyberZHG/keras-targeted-dropout> (Keras)

#### Article

- [Dropout可能要换了，Hinton等研究者提出神似剪枝的Targeted Dropout](http://www.sohu.com/a/277688850_129720)


### 3.8.4 Normalization

- BatchNormalization: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - Google2015](https://arxiv.org/abs/1502.03167)

- ReNorm: [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models - Google2017](https://arxiv.org/abs/1702.03275)

    ReNorm算法在BatchNorm算法上做了一些改进，使得模型在小批次场景中也有良好的效果。

- GroupNorm: [Group Normalization - Facebook2018](https://arxiv.org/abs/1803.08494)

    GroupNorm算法是介于LayerNorm算法和InstanceNorm算法之间的算法。它首先将通道分为许多组（group），再对每一组做归一化处理。GroupNorm算法与ReNorm算法的作用类似，都是为了解决BatchNorm算法对批次大小的依赖。

- SwitchableNorm: [Differentiable Learning-to-Normalize via Switchable Normalization - CUHK2019](https://arxiv.org/abs/1806.10779)

    SwitchableNorm算法是将BN算法、LN算法、IN算法结合起来使用，并为每个算法都赋予权重，让网络自己去学习归一化层应该使用什么方法。

- Layer Normalization: [Layer Normalization - Toronto2016](https://arxiv.org/abs/1607.06450)

    **Code**: <https://github.com/CyberZHG/keras-layer-normalization> (Keras)

- [Regularization and Optimization strategies in Deep Convolutional Neural Network - Nanyang2017](https://arxiv.org/abs/1712.04711)

    **Chinese**: [一文概览深度学习中的五大正则化方法和七大优化策略](https://baijiahao.baidu.com/s?id=1587281671012380526)

#### Article

- [BN、LN、IN、GN、SN归一化 - 2019](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247486989&idx=2&sn=29ccf3cebfb860e0286936e50f519f02)

- [常用的 Normalization 方法：BN、LN、IN、GN - 2019](https://zhuanlan.zhihu.com/p/72589565)


## 3.9 Optimization

### 3.9.1 Overview

#### Article

- 【Great】[An overview of gradient descent optimization algorithms - 2016~2018](http://ruder.io/optimizing-gradient-descent/index.html)

- [2017年深度学习优化算法最新综述 - 2017](https://www.fengiling.com/blog/view/?id=734522)

- [Gradient Boosting explained (demonstration) - 2016](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)

- [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam) - 2016](https://zhuanlan.zhihu.com/p/22252270)

- [简单的解释，让你秒懂“最优化” 问题 - 2017](https://blog.csdn.net/fnqtyr45/article/details/78775446/)

- [为什么我们更宠爱“随机”梯度下降？（SGD）- 2017](https://zhuanlan.zhihu.com/p/28060786)

- [神经网络优化算法：Dropout、梯度消失/爆炸、Adam优化算法，一篇就够了！- 2019](https://zhuanlan.zhihu.com/p/78854514)

- [在深度学习模型的优化上，梯度下降并非唯一的选择 - 2019](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247490838&idx=3&sn=eb2d9807406d7ef3aabfb6eea8ab3515)


### 3.9.2 Adam

- Adam: 

- RAdam: [On the Variance of the Adaptive Learning Rate and Beyond - Illinois2019](https://arxiv.org/abs/1908.03265v1)

#### Article

- [New State of the Art AI Optimizer - RAdam(Rectified Adam) - 2019](https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b)

    **Chinese**: [不是我们喜新厌旧，而是 RAdam 确实是好用，新的 State of the Art 优化器 RAdam](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247493046&idx=2&sn=538eaec72c6eccd92d16f3106a0d836a)


## 3.10 Initialization

### 3.10.1 Overview

Xavier Initialization is better for tanh and sigmoid, and He Initialization is better for ReLU.

#### Article

-  [Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming - 2019](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)

    神经网络中的权重初始化一览：从基础到Kaiming


### 3.10.2 Xavier

[Understanding the difficulty of training deep feedforward neural networks - Canada2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

又叫 Glorot Uniform ？

### 3.10.3 Kaiming

[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - Microsoft2015](https://arxiv.org/abs/1502.01852)


## 3.11 Capsule

#### Paper

- Capsule: [Dynamic Routing Between Capsules - Google2017](https://arxiv.org/abs/1710.09829)

    深刻而又尖锐的点出了卷积网络的硬伤。

- [Information Aggregation via Dynamic Routing for Sequence Encoding - Fudan2018](https://arxiv.org/abs/1806.01501)

    该论文的实践证明，与原有的注意力机制相比，动态路由算法确实在精度上有所提升。

#### Code

- <https://github.com/naturomics/CapsNet-Tensorflow> (Tensorflow)

- <https://github.com/gram-ai/capsule-networks> (PyTorch)

- <https://github.com/timomernick/pytorch-capsule> (PyTorch)

- <https://github.com/XifengGuo/CapsNet-Keras> (Keras)

- <https://github.com/bojone/Capsule> (Keras)

#### Practice

- <https://github.com/AidenHuen/Capsule-Text-Classification> (Keras)

    利用keras搭建的胶囊网络(capsule network文本分类模型，包含RNN、CNN、HAN等)

- <https://github.com/andyweizhao/capsule_text_classification> (Tensorflow)

#### Article

- [揭开迷雾，来一顿美味的Capsule盛宴 - 2018](https://kexue.fm/archives/4819)


## 3.12 Graph Neural Network

#### Article

- 从图(Graph)到图卷积(Graph Convolution): 漫谈图神经网络

    [一](https://zhuanlan.zhihu.com/p/108294485), [二](https://zhuanlan.zhihu.com/p/108298787), [三](https://zhuanlan.zhihu.com/p/108299847)



## 3.13 Reinforcement Learning

#### Code

- <https://github.com/dennybritz/reinforcement-learning> (Tensorflow)

    Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course

- <https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch> (PyTorch)

    PyTorch implementations of deep reinforcement learning algorithms and environments

    **Article**: [17 种深度强化学习算法的 Pytorch 实现](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247491996&idx=2&sn=db144484ed66a28377051cf06268260b)

#### Paper

- [Ranking Sentences for Extractive Summarization with Reinforcement Learning - Edinburgh2018](https://arxiv.org/abs/1802.08636)

    **Chinese**: [基于强化学习的句子摘要排序](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247487291&idx=3&sn=5f117067af1b56f5ae7afbfd0aa33ce4)

#### Tool

- [GAN Lab: Play with GANs in your browser](https://poloclub.github.io/ganlab/)



## 3.14 Others

#### Article

- [机器学习中端到端学习的本质是什么？有什么优缺点？](https://www.zhihu.com/question/349900338)

- [深度学习环境配置有哪些坑？ - 2019](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247490876&idx=5&sn=69c0a904542967d4f97f0a859e6ebcca)


