
# 3. Deep Learning

## 3.1 Overview

#### Paper

- <https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap>

    Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!

- [Recent Advances in Deep Learning: An Overview - Bangladesh2018](https://arxiv.org/abs/1807.08169v1)

    这篇综述论文列举了近年来深度学习的重要研究成果，从方法、架构、正则化、优化技术等方面进行概述。

- [The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches - 2018](https://arxiv.org/abs/1803.01164)

    **Chinese**: [从起源到具体算法，这篇深度学习综述论文送给你](https://yq.aliyun.com/articles/552398)

- <https://github.com/robertsdionne/neural-network-papers>

    Surveys, Books, Datasets, Pretrained Models, Frameworks, NLP, CNN, RNN, CRNN, Boltzman, RL, Optimization, Hardware, Clustering, etc.

- [Mathematics of Deep Learning - 2017](https://arxiv.org/abs/1712.04741)

    **Chinese**: [揭秘深度学习成功的数学原因：从全局最优性到学习表征不变性](http://www.sohu.com/a/210846652_465975)

- [Layer rotation: a surprisingly powerful indicator of generalization in deep networks? - FNRS2019](https://arxiv.org/abs/1806.01603)

    层旋转

#### Code

- 【Great!!!】<https://github.com/rasbt/deeplearning-models>

    大量深度学习架构、模型和Tips，使用Tensorflow和PyTorch两种语言Double实现！

- 【Great!!!】[超全！深度学习最常见的 26 个模型练手项目汇总](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247489258&idx=1&sn=aa6690482375bdad09cdbca80ed06068)

- <https://github.com//RedditSota/state-of-the-art-result-for-machine-learning-problems>

    This repository provides SOTA results for all machine learning problems. We do our best to keep this repository up to date.

#### Course

- Neural Network and Deep Learning - Coursera by Andrew Ng

    Language: English, Chinese, ...

    Homework: Jupyter on Coursera

    **Note**: [深度学习笔记目录](http://www.ai-start.com/dl2017/)

#### Article

- [这 28 张精炼图，将吴恩达的 deeplearning.ai 总结得恰到好处！](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247487236&idx=1&sn=4547a524c6884adfff4c13ba0287cf98)

- [如何简单形象又有趣地讲解神经网络是什么？ - 2017](https://www.zhihu.com/question/22553761#answer-9961552)

- [图文并茂的神经网络架构大盘点：从基本原理到衍生关系 - 2016](http://www.360doc.com/content/16/0915/20/13792507_591115257.shtml)

- [辨别真假数据科学家必备手册：深度学习45个基础问题（附答案）- 2017](http://mini.eastday.com/mobile/170227083107275.html)

- [过参数化、剪枝和网络结构搜索 - 2019](https://zhuanlan.zhihu.com/p/74553341)

- [神经网络多样性的意义何在？既然多层感知机在理论上已经可以拟合任何函数，为什么要有不同的形式？](https://www.zhihu.com/question/342489117)


## 3.2 CNN

#### Paper

- [What Do We Understand About Convolutional Networks? - Canada2018](https://arxiv.org/abs/1803.08834)

    **Chinese**: [94 页论文综述卷积神经网络：从基础技术到研究前景](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247493220&idx=3&sn=a27d0aaf9210f46e614c2863ed76de3d)

#### Tool

- [3D Visualization of a CNN](http://scs.ryerson.ca/~aharley/vis/conv/)

#### Article

- [YJango的卷积神经网络——介绍 - 2019](https://zhuanlan.zhihu.com/p/27642620)

- [Convolutional Neural Networks backpropagation: from intuition to derivation - 2016](https://grzegorzgwardys.wordpress.com/2016/04/22/8/)

- [机器学习原来这么有趣！第三章:图像识别【鸟or飞机】？深度学习与卷积神经网络 - 2017](https://zhuanlan.zhihu.com/p/24524583)

- [CNN详解——反向传播过程](https://blog.csdn.net/HappyRocking/article/details/80512587)

- [卷积层和分类层，哪个更重要？ - 2019](https://www.zhihu.com/question/270245936)

- [15个CNN关键回答集锦，2019校招面试必备！- 2018](https://blog.csdn.net/qq_28168421/article/details/81713457)

- [自然语言处理中CNN模型几种常见的Max Pooling操作 - 2016](http://blog.csdn.net/malefactor/article/details/51078135)

- [变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作 - 2019](https://zhuanlan.zhihu.com/p/28749411)

- [CNN调优总结 - 2019](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247487348&idx=2&sn=b321b86e29a258e807b09f6deecaea35)

- [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning - 2019](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)

    **Chinese**: [卷积有多少种？一文读懂深度学习中的各种卷积](https://mp.weixin.qq.com/s?__biz=MzUyOTU2MjE1OA==&mid=2247486088&idx=1&sn=78af01181a293551aef468baeac1ae00)

- [四张图彻底搞懂CNN反向传播算法（通俗易懂） - 2019](https://zhuanlan.zhihu.com/p/81675803)


## 3.3 RNN

#### Article

- [解决RNN的梯度消失问题 - 2019](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247489660&idx=4&sn=be100c0a984554681937dfb9126ef92a)

- [卷积网络循环网络结合-CNN+RNN - 2018](http://www.sohu.com/a/222287668_609569)


## 3.4 LSTM/GRU

### 3.4.1 LSTM

- LSTM: [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

- [Regularizing and Optimizing LSTM Language Models - Salesforce2017](https://arxiv.org/abs/1708.02182)

    LSTM优化和训练技巧 推荐阅读！

#### Article

- [Understanding LSTM Networks - 2015](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

- [Deep Learning for NLP Best Practices blog](http://ruder.io/deep-learning-nlp-best-practices/)

- [Visualizing LSTM Networks. Part I - 2018](https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7)

    **Code**: <https://github.com/asap-report/lstm-visualisation>

    **Chinese**: [可视化LSTM网络：探索「记忆」的形成](https://baijiahao.baidu.com/s?id=1596521378161967384)


#### Practice

- [如何准备用于LSTM模型的数据并进行序列预测？（附代码）](https://www.jiqizhixin.com/articles/2018-12-18-16)


### 3.4.2 AWD-LSTM

AWD-LSTM 对 LSTM 模型进行了改进，包括在隐藏层间加入 dropout ，加入词嵌入 dropout ，权重绑定等。建议**使用 AWD-LSTM 来替代 LSTM 以获得更好的模型效果**。

[Regularizing and Optimizing LSTM Language Models - 2017](https://arxiv.org/abs/1708.02182)

#### Code

- [Github by Salesforce](https://github.com/salesforce/awd-lstm-lm)
  
- [Github by fastAI](https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py)


### 3.4.3 Adversarial LSTM

[Adversarial Training Methods for Semi-Supervised Text Classification - 2016](https://arxiv.org/abs/1605.07725)

核心思想：通过对 word Embedding 上添加噪音生成对抗样本，将对抗样本以和原始样本 同样的形式喂给模型，得到一个 Adversarial Loss，通过和原始样本的 loss 相加得到新的损失，通过优化该新 的损失来训练模型，作者认为这种方法能对 word embedding 加上正则化，避免过拟合。

#### Practice

- [文本分类实战（七）—— Adversarial LSTM 模型](https://www.cnblogs.com/jiangxinyang/p/10208363.html)


### 3.4.4 Advanced LSTM

[Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition - Alibaba2017](https://arxiv.org/abs/1710.10197)


### 3.4.5 ON-LSTM

[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks - 2019](https://arxiv.org/abs/1810.09536)

**Chineses**: [翻译](https://blog.csdn.net/zhang2010hao/article/details/91976075)

#### Code

- <https://github.com/yikangshen/Ordered-Neurons>


### 3.4.6 GRU


## 3.5 Other CNN/RNN/TCN

- QRNN: [Quasi-Recurrent Neural Networks - Salesforce2016](https://arxiv.org/abs/1611.01576)

- SRU: [Simple Recurrent Units for Highly Parallelizable Recurrence - ASAPP2018](https://arxiv.org/abs/1709.02755)

    SRU单元在本质上与QRNN单元很像。从网络构建上看，SRU单元有点像QRNN单元中的一个特例，但是又比QRNN单元多了一个直连的设计。

- IndRNN: [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN - Australia2018](https://arxiv.org/abs/1803.04831)

    将IndRNN单元配合ReLu等非饱和激活函数一起使用，会使模型表现出更好的鲁棒性。

- JANET: [The unreasonable effectiveness of the forget gate - Cambridge2018](https://arxiv.org/abs/1804.04849)

- TCN: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling - 2018](https://arxiv.org/abs/1803.01271)

    **Article**: [TCNs Take Over from RNNs for NLP Predictions - 2018](https://www.datasciencecentral.com/profiles/blogs/temporal-convolutional-nets-tcns-take-over-from-rnns-for-nlp-pred)

    **Chinese**: [时间卷积网络（TCN）在 NLP 多领域发光，RNN 或将没落](https://yq.aliyun.com/articles/593033)


## 3.6 Backpropogation

#### Article

- [你看到的最直白清晰的，神经网络中的反向传播法讲解 - 2018](https://blog.csdn.net/meyh0x5vDTk48P2/article/details/79124757)

- [你真的理解反向传播吗 - 2019](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247492079&idx=4&sn=5fb740d5324e1b1c3dec83fd8599196a)

- [为什么会有梯度爆炸和梯度消失？ - 2019](https://www.zhihu.com/question/290392414/answer/951298995)

    **YAO**: 每层神经网络大体是一次矩阵乘法+一次激活函数：$h_{t+1}=\sigma (Wh_t)$，这样基于**求导的链式法则**，$\partial h_{t+1}/\partial h_t=\sigma ^`(Wh_t)W$，第一项小于1，第二项可大于1也可小于1，两者相乘有时会有收缩效果，有时会有扩张效果，前者导致梯度消失，后者导致梯度爆炸。


## 3.6 Activation

### 3.6.1 Overview

#### Paper

- [Comparing Deep Learning Activation Functions Across NLP tasks - 2019](https://arxiv.org/abs/1901.02671)

    **Code**: <https://github.com/UKPLab/emnlp2018-activation-functions>

    **Chinese**: [21种NLP任务激活函数大比拼：你一定猜不到谁赢了](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650756158&idx=2&sn=90cb49c49be078e7406539eb93561c9e)


#### Article

- [聊一聊深度学习的activation function - 2017](https://zhuanlan.zhihu.com/p/25110450)

- [从ReLU到Sinc，26种神经网络激活函数可视化 - 2017](http://www.dataguru.cn/article-12255-1.html)

- [Visualising Activation Functions in Neural Networks](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)

    **Chinese**: [26种神经网络激活函数可视化 - 2017](https://www.jiqizhixin.com/articles/2017-10-10-3)

- [ReLu(Rectified Linear Units)激活函数 - 2015](https://www.cnblogs.com/neopenx/p/4453161.html)

- [Softmax](http://neuralnetworksanddeeplearning.com/chap3.html#softmax)

- [Understanding Activation Functions in Deep Learning - 2017](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)

- [请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function?](https://www.zhihu.com/question/29021768)


### 3.6.2 ReLU

- ReLU: 正值时斜率为1，负值时斜率为0

- Leaky ReLU: 给所有负值赋予一个固定的非零斜率

- PReLU: Parametric ReLU，负值对应的斜率是根据数据来获得最优值

- RReLU: Randomized ReLU，负值对应的斜率在训练时是均匀分布中随机的，在测试时是固定的

有学者将当前最优的两类CNN结合不同的激活函数在CIFAR-10,CIFAR-100和NDSB数据集上做实验，评价四种激活函数的优劣。 实验结果表明Leaky ReLU取较大的alpha准确率更好。Parametric ReLU很容易在小数据集上过拟合（训练集上错误率最低，测试集上不理想），但依然比ReLU好。RReLU效果较好，实验表明它可以克服模型过拟合，这可能由于alpha选择的随机性。在实践中， Parametric ReLU 和 Randomized ReLU 都是可取的。


## 3.7 Loss Function

#### Paper

- [Visualizing the Loss Landscape of Neural Nets - Maryland2017](https://arxiv.org/abs/1712.09913)

    **Chinese**: [马里兰大学论文：可视化神经网络的损失函数](https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/78966979)

#### Library

- <https://github.com/CyberZHG/keras-losses> (Keras)

#### Article

- [PyTorch中十九种损失函数，你能认识几个？](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247487076&idx=1&sn=cb7844e2e0b2d82cbb15db4ecf906e3a)

- [从最优化的角度看待Softmax损失函数 - 2019](https://zhuanlan.zhihu.com/p/45014864)

- [Softmax理解之二分类与多分类 - 2019](https://zhuanlan.zhihu.com/p/45368976)

- [神经网络中，设计loss function有哪些技巧](https://www.zhihu.com/question/268105631)

- [深度学习中loss和accuracy的关系](https://www.zhihu.com/question/264892967)


## 3.8 Regularization

### 3.8.1 Overview

#### Article

- [An Overview of Regularization Techniques in Deep Learning (with Python code) - 2018](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)

    **Chinese**: [深度学习中的正则化技术概述（附Python+keras实现代码）](https://mp.weixin.qq.com/s?__biz=MzUyOTU2MjE1OA==&mid=2247485921&idx=1&sn=b5a9320c0e7a89679de7695f3406bec7)


### 3.8.2 L1 & L2

- L1: 取模型所有参数的绝对值 $\lambda|W|$ 作为惩罚项加入到目标函数中。L1会使得权重向量变得稀疏(非常接近零)

- L2: 最常用，取模型所有参数的平方级 $\lambda W^2 /2$ 作为惩罚项加入到目标函数中。L2对尖峰向量的惩罚很强，并且倾向于分散权重的向量。


### 3.8.3 Dropout

- Dropout: 训练时对神经网络节点进行子采样(有些子节点在向前向后连接时不考虑)，测试时Dropout不使用

    - [Improving neural networks by preventing co-adaptation of feature detectors - Toronto2012](https://arxiv.org/abs/1207.0580)
        
        提出了Dropout

    - [ImageNet Classification with Deep Convolutional Neural Networks - Toronto2012](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)
    
        提出了AlexNet，使用了Dropout
    
    - [Improving Neural Networks with Dropout - Toronto2013](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)

    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting - Toronto2014](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
    
    - [Dropout as data augmentation - Canada2016](https://arxiv.org/abs/1506.08700)

- [Recurrent Neural Network Regularization - NYU2015](https://arxiv.org/abs/1409.2329)

    How to correctly apply Dropout to RNNs  推荐阅读！

- Targeted Dropout: [Targeted Dropout - OX2018](https://openreview.net/pdf?id=HkghWScuoQ)

    **Code**: <https://github.com/for-ai/TD/tree/master/models>

    **Code**: <https://github.com/CyberZHG/keras-targeted-dropout> (Keras)

#### Article

- [Dropout可能要换了，Hinton等研究者提出神似剪枝的Targeted Dropout](http://www.sohu.com/a/277688850_129720)


### 3.8.4 Batch Normalization

- Layer Normalization: [Layer Normalization - Toronto2016](https://arxiv.org/abs/1607.06450)

    **Code**: <https://github.com/CyberZHG/keras-layer-normalization> (Keras)

- BatchNormalization: [Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - Google2015](https://arxiv.org/abs/1502.03167)

- ReNorm: [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models - Google2017](https://arxiv.org/abs/1702.03275)

    ReNorm算法在BatchNorm算法上做了一些改进，使得模型在小批次场景中也有良好的效果。

- GroupNorm: [Group Normalization - Facebook2018](https://arxiv.org/abs/1803.08494)

    GroupNorm算法是介于LayerNorm算法和InstanceNorm算法之间的算法。它首先将通道分为许多组（group），再对每一组做归一化处理。GroupNorm算法与ReNorm算法的作用类似，都是为了解决BatchNorm算法对批次大小的依赖。

- SwitchableNorm: [Differentiable Learning-to-Normalize via Switchable Normalization - CUHK2019](https://arxiv.org/abs/1806.10779)

    SwitchableNorm算法是将BN算法、LN算法、IN算法结合起来使用，并为每个算法都赋予权重，让网络自己去学习归一化层应该使用什么方法。

- [Regularization and Optimization strategies in Deep Convolutional Neural Network - Nanyang2017](https://arxiv.org/abs/1712.04711)

    **Chinese**: [一文概览深度学习中的五大正则化方法和七大优化策略](https://baijiahao.baidu.com/s?id=1587281671012380526)

#### Article

- [BN、LN、IN、GN、SN归一化 - 2019](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247486989&idx=2&sn=29ccf3cebfb860e0286936e50f519f02)

- [常用的 Normalization 方法：BN、LN、IN、GN - 2019](https://zhuanlan.zhihu.com/p/72589565)


## 3.9 Optimization

### 3.9.1 Overview

#### Article

- [An overview of gradient descent optimization algorithms - 2016](http://ruder.io/optimizing-gradient-descent/index.html)

- [2017年深度学习优化算法最新综述 - 2017](https://www.fengiling.com/blog/view/?id=734522)

- [Gradient Boosting explained (demonstration) - 2016](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)

- [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam) - 2016](https://zhuanlan.zhihu.com/p/22252270)

- [简单的解释，让你秒懂“最优化” 问题 - 2017](https://blog.csdn.net/fnqtyr45/article/details/78775446/)

- [为什么我们更宠爱“随机”梯度下降？（SGD）- 2017](https://zhuanlan.zhihu.com/p/28060786)

- [神经网络优化算法：Dropout、梯度消失/爆炸、Adam优化算法，一篇就够了！- 2019](https://zhuanlan.zhihu.com/p/78854514)

- [在深度学习模型的优化上，梯度下降并非唯一的选择 - 2019](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247490838&idx=3&sn=eb2d9807406d7ef3aabfb6eea8ab3515)


### 3.9.2 Adam

- Adam: 

- RAdam: [On the Variance of the Adaptive Learning Rate and Beyond - Illinois2019](https://arxiv.org/abs/1908.03265v1)

#### Article

- [New State of the Art AI Optimizer - RAdam(Rectified Adam) - 2019](https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b)

    **Chinese**: [不是我们喜新厌旧，而是 RAdam 确实是好用，新的 State of the Art 优化器 RAdam](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247493046&idx=2&sn=538eaec72c6eccd92d16f3106a0d836a)


## 3.10 Initialization

### 3.10.1 Overview

Xavier Initialization is better for tanh and sigmoid, and He Initialization is better for ReLU.

#### Article

-  [Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming - 2019](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)

    神经网络中的权重初始化一览：从基础到Kaiming


### 3.10.2 Xavier

[Understanding the difficulty of training deep feedforward neural networks - Canada2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

又叫 Glorot Uniform ？

### 3.10.3 Kaiming

[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - Microsoft2015](https://arxiv.org/abs/1502.01852)


## 3.11 Capsule

#### Paper

- Capsule: [Dynamic Routing Between Capsules - Google2017](https://arxiv.org/abs/1710.09829)

    深刻而又尖锐的点出了卷积网络的硬伤。

- [Information Aggregation via Dynamic Routing for Sequence Encoding - Fudan2018](https://arxiv.org/abs/1806.01501)

    该论文的实践证明，与原有的注意力机制相比，动态路由算法确实在精度上有所提升。

#### Code

- <https://github.com/naturomics/CapsNet-Tensorflow> (Tensorflow)

- <https://github.com/gram-ai/capsule-networks> (PyTorch)

- <https://github.com/timomernick/pytorch-capsule> (PyTorch)

- <https://github.com/XifengGuo/CapsNet-Keras> (Keras)

- <https://github.com/bojone/Capsule> (Keras)

- <https://github.com/AidenHuen/Capsule-Text-Classification> (Keras)

    利用keras搭建的胶囊网络(capsule network文本分类模型，包含RNN、CNN、HAN等)


#### Article

- [揭开迷雾，来一顿美味的Capsule盛宴 - 2018](https://kexue.fm/archives/4819)


## 3.12 GAN

#### Tool

- [GAN Lab: Play with GANs in your browser](https://poloclub.github.io/ganlab/)


## 3.13 Reinforcement Learning

#### Code

- <https://github.com/dennybritz/reinforcement-learning> (Tensorflow)

    Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course

- <https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch> (PyTorch)

    PyTorch implementations of deep reinforcement learning algorithms and environments

    **Article**: [17 种深度强化学习算法的 Pytorch 实现](https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&mid=2247491996&idx=2&sn=db144484ed66a28377051cf06268260b)

#### Paper

- [Ranking Sentences for Extractive Summarization with Reinforcement Learning - Edinburgh2018](https://arxiv.org/abs/1802.08636)

    **Chinese**: [基于强化学习的句子摘要排序](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247487291&idx=3&sn=5f117067af1b56f5ae7afbfd0aa33ce4)


## 3.14 Others

#### Article

- [机器学习中端到端学习的本质是什么？有什么优缺点？](https://www.zhihu.com/question/349900338)

